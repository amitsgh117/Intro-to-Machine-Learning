{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "190117.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_jWlyEJ0IML"
      },
      "source": [
        "**Q1.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iynygnrw9vqu"
      },
      "source": [
        "import numpy as np\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_9bBm7s-Fss"
      },
      "source": [
        "# Gradient Descent Algorithm (given in question)\n",
        "\n",
        "def gradient_descent(gradient,init_,learn_rate, n_iter=50, tol=1e-06):\n",
        "  x = init_\n",
        "  for _ in range(n_iter):\n",
        "    delta = (-1)*learn_rate*gradient(x)\n",
        "    if np.all(np.abs(delta) <= tol):\n",
        "      break\n",
        "    x += delta\n",
        "  return round(x*1000)/1000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kiKynjynBAD7",
        "outputId": "a5bb5b74-9102-4e54-8120-579f890ebdd1"
      },
      "source": [
        "# Gradient for  x^2 + 3x + 4 is 2x + 3\n",
        "# Taking initial value as 0 and learning rate 0.2:\n",
        "\n",
        "x = gradient_descent(gradient=lambda v: 2 * v + 3, init_=0, learn_rate=0.2)\n",
        "print(\"Minimum occurs at x =\", x) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Minimum occurs at x = -1.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7p48e4gJBDRC",
        "outputId": "1f2cde10-5bfa-452c-eb7b-f87dc802cb2a"
      },
      "source": [
        "# Gradient for  x^4 - 3x^2 + 2x is 4x^3 - 6x + 2\n",
        "# Taking initial value as 0 and learning rate 0.05:\n",
        "\n",
        "x = gradient_descent(gradient=lambda v: 4 * v**3 - 6 * v + 2, init_=0, learn_rate=0.05)\n",
        "print(\"Minimum occurs at x =\", x) \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Minimum occurs at x = -1.366\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AaoJ2DU4GH8O"
      },
      "source": [
        "# Cost function = mean squared error\n",
        "\n",
        "def mse(X, y, a, b):\n",
        "  N = float(len(y))\n",
        "  return (1/N) * np.sum((y - a*X - b)**2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6bLCd0wOSuVc"
      },
      "source": [
        "# To find the gradient we would take derivative of cost function wrt a and b\n",
        "\n",
        "def gradient(X,y,a,b):\n",
        "  N = float(len(y))\n",
        "  y_current = (a * X) + b\n",
        "  a_gradient = -(2/N) * sum(X * (y - y_current))\n",
        "  b_gradient = -(2/N) * sum(y - y_current)\n",
        "  return (a_gradient,b_gradient)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owAZqCihHq8-"
      },
      "source": [
        "# Linear Regression using gradient descent\n",
        "\n",
        "def linear_regression(X, y, a, b, n_iter, learn_rate, tot=1e-06):\n",
        "  N = float(len(y))\n",
        "  for _ in range(n_iter):\n",
        "    a_gradient, b_gradient = gradient(X, y, a, b)\n",
        "    err = mse(X, y, a, b)\n",
        "    if(err <= tot):\n",
        "      break\n",
        "    a = a - (learn_rate * a_gradient)\n",
        "    b = b - (learn_rate * b_gradient)\n",
        "  return a, b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XGcdPNKIczt"
      },
      "source": [
        "# Generating artificial data for this regression according to the given protocol\n",
        "\n",
        "np.random.seed(0)\n",
        "X = 2.5 * np.random.randn(10000) + 1.5\n",
        "res = 1.5 * np.random.randn(10000)\n",
        "y = 2 + 0.3*X + res"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ukF7LaWIuWn",
        "outputId": "a48c509b-7c3a-4eae-e26b-753aff334369"
      },
      "source": [
        "# Finding optimal parameters (a,b) for y = ax + b\n",
        "# Initializing (a,b) with (0,0)\n",
        "# Keeping no of iterations 1000 and learning rate 0.01\n",
        "\n",
        "linear_regression(X, y, a=0, b=0, n_iter=1000, learn_rate=0.01)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.29531870750081424, 2.0232875000170694)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jg2EOqHEKQ5O"
      },
      "source": [
        "# Implementation of minibatch stochastic gradient descent\n",
        "\n",
        "# Function to create mini batch by randomly taking subset from original data of a given batch size\n",
        "def iterate_minibatches(X, y, batchsize, shuffle=False):\n",
        "  assert X.shape[0] == y.shape[0]\n",
        "  if shuffle:\n",
        "    indices = np.arange(X.shape[0])\n",
        "    np.random.shuffle(indices)\n",
        "  for start_idx in range(0, X.shape[0], batchsize):\n",
        "    end_idx = min(start_idx + batchsize, X.shape[0])\n",
        "    if shuffle:\n",
        "      excerpt = indices[start_idx:end_idx]\n",
        "    else:\n",
        "      excerpt = slice(start_idx, end_idx)\n",
        "    yield X[excerpt], y[excerpt]\n",
        "\n",
        "# Function to return optimal (a,b) using mini batch stochastic gradient descent\n",
        "def mini_sgd(X, y, a, b, n_iter, batchsize, learn_rate ,tol=1e-06 ):\n",
        "  for _ in range(n_iter):\n",
        "    for batch in iterate_minibatches(X, y, batchsize , shuffle=True):\n",
        "      x_batch, y_batch = batch\n",
        "      a_gradient, b_gradient = gradient(x_batch, y_batch, a, b)\n",
        "      err = mse(X, y, a, b)\n",
        "      if (err <= tol):\n",
        "        break\n",
        "      a = a - (learn_rate * a_gradient)\n",
        "      b = b - (learn_rate * b_gradient)\n",
        "  return (a, b)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EprI1m4CLYTy",
        "outputId": "2b8c9b3b-5994-400e-f729-a5ca2ebe088d"
      },
      "source": [
        "# Taking initial value of a and b as 0, n_iter as 100, learning rate 0.01 and batch size as 10:\n",
        "\n",
        "mini_sgd(X, y, a=0, b=0, n_iter=100, batchsize=10, learn_rate=0.01)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.26277502508394496, 2.041715532939543)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nFM8yzphNN3V",
        "outputId": "c59a6718-03fc-437e-c040-3bb69a7d24d4"
      },
      "source": [
        "# Comparing time for both functions, keeping batch size and n_iter such that no of times a and b is getting updated in both the functions is same\n",
        "\n",
        "start = time.time()\n",
        "a, b = linear_regression(X, y, a=0, b=0, n_iter=1000, learn_rate=0.01)\n",
        "finish = time.time()\n",
        "print(finish-start)\n",
        "\n",
        "start = time.time()\n",
        "a, b = mini_sgd(X, y, a=0, b=0, n_iter=250, batchsize=2500, learn_rate=0.01)\n",
        "finish = time.time()\n",
        "print(finish-start)\n",
        "\n",
        "# mini batch stochastic gradient descent works faster"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.543976068496704\n",
            "1.0032060146331787\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FWiBQxiHNjlo",
        "outputId": "f9e78564-c103-4086-d44a-2a5ca42991e6"
      },
      "source": [
        "# Calculating TIME TAKEN and ERROR for different values of batch size\n",
        "\n",
        "start = time.time()\n",
        "a, b = mini_sgd(X, y, a=0, b=0, n_iter=2, batchsize=20, learn_rate=0.01)\n",
        "finish = time.time()\n",
        "print(finish-start, mse(X, y, a, b))\n",
        "\n",
        "start = time.time()\n",
        "a, b = mini_sgd(X, y, a=0, b=0, n_iter=5, batchsize=50, learn_rate=0.01)\n",
        "finish = time.time()\n",
        "print(finish-start, mse(X, y, a, b))\n",
        "\n",
        "start = time.time()\n",
        "a, b = mini_sgd(X, y, a=0, b=0, n_iter=8, batchsize=80, learn_rate=0.01)\n",
        "finish = time.time()\n",
        "print(finish-start, mse(X, y, a, b))\n",
        "\n",
        "start = time.time()\n",
        "a, b = mini_sgd(X, y, a=0, b=0, n_iter=10, batchsize=100, learn_rate=0.01)\n",
        "finish = time.time()\n",
        "print(finish-start, mse(X, y, a, b))\n",
        "\n",
        "start = time.time()\n",
        "a, b = mini_sgd(X, y, a=0, b=0, n_iter=25, batchsize=250, learn_rate=0.01)\n",
        "finish = time.time()\n",
        "print(finish-start, mse(X, y, a, b))\n",
        "\n",
        "start = time.time()\n",
        "a, b = mini_sgd(X, y, a=0, b=0, n_iter=50, batchsize=500, learn_rate=0.01)\n",
        "finish = time.time()\n",
        "print(finish-start, mse(X, y, a, b))\n",
        "\n",
        "start = time.time()\n",
        "a, b = mini_sgd(X, y, a=0, b=0, n_iter=100, batchsize=1000, learn_rate=0.01)\n",
        "finish = time.time()\n",
        "print(finish-start, mse(X, y, a, b))\n",
        "\n",
        "start = time.time()\n",
        "a, b = mini_sgd(X, y, a=0, b=0, n_iter=500, batchsize=5000, learn_rate=0.01)\n",
        "finish = time.time()\n",
        "print(finish-start, mse(X, y, a, b))\n",
        "\n",
        "start = time.time()\n",
        "a, b = mini_sgd(X, y, a=0, b=0, n_iter=1000, batchsize=10000, learn_rate=0.01)\n",
        "finish = time.time()\n",
        "print(finish-start, mse(X, y, a, b))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.05029487609863281 2.2442834591340945\n",
            "0.06282186508178711 2.222418967849953\n",
            "0.08701562881469727 2.2215830657130815\n",
            "0.09013795852661133 2.220800358091458\n",
            "0.14704537391662598 2.21883369153521\n",
            "0.24843859672546387 2.2187365897696845\n",
            "0.42586636543273926 2.2187381915691406\n",
            "1.9597618579864502 2.218721114255274\n",
            "3.8057632446289062 2.2187210094383047\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Fb5gCT8mMP8"
      },
      "source": [
        "# Observation:\n",
        "# Time taken usually increases with increase in batch size.\n",
        "# In terms of accuracy, higher batch size works best in this case.\n",
        "# Hence there will be some optimum batch size that has both least error and takes less time.\n",
        "# Batch size ~ 250-500 will work the best both in terms of accuracy and time."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UouGyNq4kLXf"
      },
      "source": [
        "**Q2.**\n",
        "\n",
        "**(i)** **The probability that someone has both cold and a fever:** \n",
        "$$= P(Cold) \\times P(Fever|Cold)$$ \n",
        "$$= 0.02 \\times 0.307$$ \n",
        "$$= 0.00614$$\n",
        "\n",
        "\\\n",
        "\n",
        "\\\n",
        "\n",
        "**(ii) Firstly, the probability that someone may have lung disease:** $$= 0.2 \\times 0.1009 + 0.8 \\times 0.001 = 0.02098$$\n",
        "\n",
        "**The possibilities of causes of cough can be:**\n",
        "\n",
        "*   Lung disease along with cold $(C_1)$ : $$P(C_1) = 0.02098 \\times\n",
        " 0.02 = 0.0004196$$\n",
        "*   Lung disease but not cold $(C_2)$ : $$P(C_2) = 0.02098 \\times\n",
        " (1 - 0.02) =  0.0205604$$\n",
        "*   Cold but not lung disease $(C_3)$ : $$P(C_3) = (1 - 0.02098) \\times\n",
        " 0.02 = 0.0195804$$\n",
        "*   Neither lung disease nor cold $(C_4)$ : $$P(C_4) = (1 - 0.02098) \\times\n",
        " (1 - 0.02) = 0.9594396$$\n",
        "\n",
        " \\\n",
        "\n",
        "**Probability that someone has a cough $(C)$ :** $$P(C) = P(C|C_1)P(C_1) + P(C|C_2)P(C_2) + P(C|C_3)P(C_3) + P(C|C_4)P(C_4)$$\n",
        "\n",
        "$$= 0.7525 \\times 0.0004196 + 0.505 \\times 0.0205604 + 0.505 \\times 0.0195804 + 0.01 \\times 0.9594396$$\n",
        "$$= 0.030181249$$\n",
        "\n",
        "\\\n",
        "\n",
        "**The probability that someone who has a cough has a cold:** $$= \\frac{P(C|C1)P(C1) + P(C|C3)P(C3)}{P(C)}$$\n",
        "\n",
        "$$= \\frac{(0.7525 \\times 0.0004196 + 0.505 \\times 0.0195804)}{0.030181249} = 0.33808577637$$\n",
        "\n",
        "\\\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grSJZ7xXpvDp"
      },
      "source": [
        "**Q3.**\n",
        "\n",
        "For k-sided multinomial distribution, the probability is defined as:\n",
        "\n",
        "$$P(x|\\theta) = n! \\prod_{i=1}^{k} \\frac{{\\theta_i}^{x_i}}{x_i!}$$\n",
        "\n",
        "where,\n",
        "\n",
        "\n",
        "*   $\\sum^{k}_{i=1} x_i = n$\n",
        "*   $\\sum^{k}_{i=1} \\theta_i = 1$\n",
        "*   $\\theta_i \\geq 0$ and $x_i  \\geq 0, \\forall i \\in [k] $\n",
        "\n",
        "The log-likelihood of this probability is \n",
        "$$ LL(\\theta) = log(n!) + \\sum^{k}_{i=1} x_i log (\\theta_i) - \\sum^{k}_{i=1} log(x_i!)$$\n",
        "\n",
        "Let the constraint is defined by:\n",
        "$$ g(\\theta) = (\\sum^{k}_{i=1} \\theta_i) - 1 = 0 $$\n",
        "\n",
        "So our optimization problem becomes\n",
        "$$ L(\\theta, \\lambda) = LL(\\theta) - \\lambda g(\\theta) $$\n",
        "$$ L(\\theta, \\lambda) = log(n!) + \\sum^{k}_{i=1} x_i log (\\theta_i) - \\sum^{k}_{i=1} log(x_i!) - \\lambda (\\sum^{k}_{i=1} \\theta_i) + \\lambda $$\n",
        "\n",
        "\\\n",
        "\n",
        "\n",
        "Now $\\forall i \\in [k],$ $$\\frac{\\partial L}{\\partial \\theta_i} = \\frac{x_i}{\\theta_i} - \\lambda = 0  \\Rightarrow \\theta_i = \\frac{x_i}{\\lambda} $$\n",
        "And, $$(\\sum^{k}_{i=1} \\theta_i)  -1 = 0 \\Rightarrow \\sum^{k}_{i=1} \\frac{x_i}{\\lambda} = 1 \\Rightarrow \\lambda = \\sum^{k}_{i=1} x_i $$\n",
        "$$\\therefore \\theta_i = \\frac{x_i}{\\sum^{k}_{i=1} x_i}$$\n",
        "\n",
        "\\\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    }
  ]
}